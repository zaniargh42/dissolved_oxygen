{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense  ,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from lazypredict.Supervised import LazyRegressor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data():\n",
    "    train=pd.read_excel('Data set - Tisa.xlsx',sheet_name='Training set 2011-2015')\n",
    "    train.columns=['temperature', 'solids', 'dissolved_oxygen', 'pH','electrical', 'NH4', 'NO2', 'NO3', 'TN', 'PO4P', 'BOD5']\n",
    "    train=train.drop(list(train[train.isna().any(axis=1)].index),axis=0)\n",
    "\n",
    "    test=pd.read_excel('Data set - Tisa.xlsx',sheet_name='Testing set 2016-2019 ')\n",
    "    test.columns=['temperature', 'solids', 'dissolved_oxygen', 'pH','electrical', 'NH4', 'NO2', 'NO3', 'TN', 'PO4P', 'BOD5']\n",
    "    test=test.drop(list(test[test.isna().any(axis=1)].index),axis=0)\n",
    "\n",
    "    print(train.shape,test.shape)\n",
    "    return train , test\n",
    "\n",
    "def _prepare_data(data):\n",
    "    X_train=data.drop(['dissolved_oxygen'],axis=1)\n",
    "    y_train=data.dissolved_oxygen\n",
    "    return X_train , y_train\n",
    "\n",
    "def _scale_data(X_train,y_train):\n",
    "    X_scaler=StandardScaler()\n",
    "    X_train_scaled=X_scaler.fit_transform(X_train)\n",
    "    y_scaler=StandardScaler()\n",
    "    y_train_scaled=y_scaler.fit_transform(np.array(y_train).reshape(-1,1))\n",
    "    print(X_train_scaled.shape, y_train_scaled.shape)\n",
    "    return X_train_scaled,y_train_scaled , X_scaler , y_scaler\n",
    "\n",
    "def r_squared(y, y_hat):\n",
    "    y_bar = y.mean()\n",
    "    ss_tot = ((y-y_bar)**2).sum()\n",
    "    ss_res = ((y-y_hat)**2).sum()\n",
    "    return 1 - (ss_res/ss_tot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(X_train[0].shape[0],), kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# model.fit(X_train, y_train,epochs=10,batch_size=8)\n",
    "\n",
    "KerasRegressor(model=model, epochs=100, batch_size=5, verbose=0).fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have some missing values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonul_train=train.drop(list(train[train.isna().any(axis=1)].index),axis=0)\n",
    "X_train,y_train=_prepare_data(nonul_train)\n",
    "X_train,y_train , X_scaler , y_scaler=_scale_data(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonul_test=test.drop(list(test[test.isna().any(axis=1)].index),axis=0)\n",
    "X_test,y_test=_prepare_data(nonul_test)\n",
    "\n",
    "X_test=X_scaler.transform(X_test)\n",
    "y_test=y_scaler.transform(np.array(y_test).reshape(-1,1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create the model and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compile_model(X_train,y_train,X_test,y_test,epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X_train[0].shape[0], input_shape=(X_train[0].shape[0],), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\n",
    "    history=model.history\n",
    "    y_pred=model.predict(X_test)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    return history , r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history , r2=_compile_model(X_train,y_train,X_test,y_test,80,32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size itrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all=[]\n",
    "for item in tqdm([8,16,32,64,128,256,512]):\n",
    "    history , r2=_compile_model(X_train,y_train,X_test,y_test,80,item)\n",
    "    r2_all.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(7),r2_all)\n",
    "plt.xticks(range(7),[8,16,32,64,128,256,512])\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('R_squared')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_all=[]\n",
    "for item in tqdm(range(150,200,10)):\n",
    "    history , r2=_compile_model(X_train,y_train,X_test,y_test,item,8)\n",
    "    r2_all.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(r2_all)),r2_all)\n",
    "plt.xticks(range(len(r2_all)),range(10,200,10))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('R_squared')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X_test[:100,:]\n",
    "y=y_test[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history , r2=_compile_model(X_train,y_train,x,y,100,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model\n",
    "it gives us r2=0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n"
     ]
    }
   ],
   "source": [
    "train , test =_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 10) (605, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=_prepare_data(train)\n",
    "X_train,y_train , X_scaler , y_scaler=_scale_data(X_train,y_train)\n",
    "\n",
    "X_test,y_test=_prepare_data(test)\n",
    "# to set a new scaler\n",
    "# X_test_scaler=StandardScaler()\n",
    "# y_test_scaler=StandardScaler()\n",
    "# X_test=X_test_scaler.fit_transform(X_test)\n",
    "# y_test=y_test_scaler.fit_transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "#to scale by the train scaler\n",
    "X_test=X_scaler.transform(X_test)\n",
    "y_test=y_scaler.transform(np.array(y_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "76/76 [==============================] - 2s 13ms/step - loss: 1.0000 - val_loss: 0.7754\n",
      "Epoch 2/70\n",
      "76/76 [==============================] - 1s 8ms/step - loss: 1.0000 - val_loss: 0.7754\n",
      "Epoch 3/70\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 1.0000 - val_loss: 0.7753\n",
      "Epoch 4/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.9996 - val_loss: 0.7747\n",
      "Epoch 5/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.9984 - val_loss: 0.7725\n",
      "Epoch 6/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.9946 - val_loss: 0.7670\n",
      "Epoch 7/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.9857 - val_loss: 0.7537\n",
      "Epoch 8/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.9677 - val_loss: 0.7302\n",
      "Epoch 9/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.9396 - val_loss: 0.6989\n",
      "Epoch 10/70\n",
      "76/76 [==============================] - 0s 7ms/step - loss: 0.9089 - val_loss: 0.6662\n",
      "Epoch 11/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.8797 - val_loss: 0.6377\n",
      "Epoch 12/70\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.8505 - val_loss: 0.6093\n",
      "Epoch 13/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.8203 - val_loss: 0.5833\n",
      "Epoch 14/70\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.7919 - val_loss: 0.5574\n",
      "Epoch 15/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.7663 - val_loss: 0.5402\n",
      "Epoch 16/70\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.7497 - val_loss: 0.5290\n",
      "Epoch 17/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7393 - val_loss: 0.5231\n",
      "Epoch 18/70\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.7341 - val_loss: 0.5208\n",
      "Epoch 19/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7305 - val_loss: 0.5184\n",
      "Epoch 20/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7275 - val_loss: 0.5174\n",
      "Epoch 21/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7258 - val_loss: 0.5168\n",
      "Epoch 22/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 0.5160\n",
      "Epoch 23/70\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.7220 - val_loss: 0.5156\n",
      "Epoch 24/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7206 - val_loss: 0.5152\n",
      "Epoch 25/70\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.7195 - val_loss: 0.5149\n",
      "Epoch 26/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7185 - val_loss: 0.5148\n",
      "Epoch 27/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 0.5148\n",
      "Epoch 28/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7156 - val_loss: 0.5147\n",
      "Epoch 29/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 0.5147\n",
      "Epoch 30/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7132 - val_loss: 0.5148\n",
      "Epoch 31/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7127 - val_loss: 0.5147\n",
      "Epoch 32/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7113 - val_loss: 0.5150\n",
      "Epoch 33/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7108 - val_loss: 0.5151\n",
      "Epoch 34/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7098 - val_loss: 0.5152\n",
      "Epoch 35/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7087 - val_loss: 0.5156\n",
      "Epoch 36/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7084 - val_loss: 0.5155\n",
      "Epoch 37/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 0.5157\n",
      "Epoch 38/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7058 - val_loss: 0.5155\n",
      "Epoch 39/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 0.5165\n",
      "Epoch 40/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7044 - val_loss: 0.5167\n",
      "Epoch 41/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7031 - val_loss: 0.5167\n",
      "Epoch 42/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7030 - val_loss: 0.5167\n",
      "Epoch 43/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 0.5172\n",
      "Epoch 44/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7010 - val_loss: 0.5175\n",
      "Epoch 45/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7000 - val_loss: 0.5177\n",
      "Epoch 46/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.7002 - val_loss: 0.5180\n",
      "Epoch 47/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6987 - val_loss: 0.5185\n",
      "Epoch 48/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6978 - val_loss: 0.5180\n",
      "Epoch 49/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6975 - val_loss: 0.5189\n",
      "Epoch 50/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6967 - val_loss: 0.5189\n",
      "Epoch 51/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6956 - val_loss: 0.5195\n",
      "Epoch 52/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6945 - val_loss: 0.5198\n",
      "Epoch 53/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6949 - val_loss: 0.5205\n",
      "Epoch 54/70\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.5203\n",
      "Epoch 55/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6926 - val_loss: 0.5219\n",
      "Epoch 56/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6920 - val_loss: 0.5217\n",
      "Epoch 57/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6916 - val_loss: 0.5222\n",
      "Epoch 58/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6903 - val_loss: 0.5227\n",
      "Epoch 59/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6894 - val_loss: 0.5232\n",
      "Epoch 60/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6885 - val_loss: 0.5234\n",
      "Epoch 61/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6878 - val_loss: 0.5248\n",
      "Epoch 62/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6872 - val_loss: 0.5249\n",
      "Epoch 63/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6865 - val_loss: 0.5253\n",
      "Epoch 64/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6858 - val_loss: 0.5245\n",
      "Epoch 65/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6850 - val_loss: 0.5268\n",
      "Epoch 66/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6839 - val_loss: 0.5268\n",
      "Epoch 67/70\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 0.6837 - val_loss: 0.5278\n",
      "Epoch 68/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6830 - val_loss: 0.5271\n",
      "Epoch 69/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6822 - val_loss: 0.5294\n",
      "Epoch 70/70\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 0.6815 - val_loss: 0.5298\n",
      "15/15 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.31630193003044427"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _compile_model(X_train,y_train,X_test,y_test,epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(35, input_shape=(X_train[0].shape[0],),kernel_initializer='RandomUniform', activation='relu'))\n",
    "    model.add(Dense(25,kernel_initializer='RandomUniform', activation='relu'))\n",
    "    model.add(Dense(12,kernel_initializer='RandomUniform', activation='relu'))\n",
    "    model.add(Dense(6,kernel_initializer='RandomUniform', activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='RandomUniform',activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(0.0001))\n",
    "    model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\n",
    "    \n",
    "    history=model.history\n",
    "    y_pred=model.predict(X_test)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    return history , r2 , y_pred\n",
    "history , r2 , y_pred=_compile_model(X_train,y_train,X_test,y_test,70,8)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.show()\n",
    "r2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=_load_data()\n",
    "\n",
    "fake=pd.DataFrame([[1,2,3,4,5,6,7,8,9,10,11]],columns=train.columns,index=range(2000))\n",
    "fake.columns\n",
    "for item in fake.columns:\n",
    "    fake[item]=np.random.random(2000)\n",
    "fake['fake']=np.ones(len(fake))\n",
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fake']=np.zeros(len(train))\n",
    "df=pd.concat([fake,train],axis=0)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X=df.drop(['fake'],axis=1)\n",
    "y=df.fake\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X['fake']=y\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(1,len(df[df.fake==1]))\n",
    "plt.bar(0,len(df[df.fake==0]))\n",
    "plt.show()\n",
    "plt.bar(1,len(X[X.fake==1]))\n",
    "plt.bar(0,len(X[X.fake==0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=X[X.fake==0]\n",
    "train.drop(['fake'],axis=1,inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train=_prepare_data(train)\n",
    "X_train,y_train , X_scaler , y_scaler=_scale_data(X_train,y_train)\n",
    "\n",
    "X_test,y_test=_prepare_data(test)\n",
    "# to set a new scaler\n",
    "# X_test_scaler=StandardScaler()\n",
    "# y_test_scaler=StandardScaler()\n",
    "\n",
    "# X_test=X_test_scaler.fit_transform(X_test)\n",
    "# y_test=y_test_scaler.fit_transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "#to scale by the train scaler\n",
    "X_test=X_scaler.transform(X_test)\n",
    "y_test=y_scaler.transform(np.array(y_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compile_model(X_train,y_train,X_test,y_test,epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(X_train[0].shape[0],),kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(15,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0001))\n",
    "    model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\n",
    "    \n",
    "    history=model.history\n",
    "    y_pred=model.predict(X_test)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    return history , r2 , y_pred\n",
    "\n",
    "history , r2 , y_pred=_compile_model(X_train,y_train,X_test,y_test,100,8)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.show()\n",
    "print('R_squared is:',r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n"
     ]
    }
   ],
   "source": [
    "train , test =_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 10) (605, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=_prepare_data(train)\n",
    "X_train,y_train , X_scaler , y_scaler=_scale_data(X_train,y_train)\n",
    "\n",
    "X_test,y_test=_prepare_data(test)\n",
    "# to set a new scaler\n",
    "X_test_scaler=StandardScaler()\n",
    "y_test_scaler=StandardScaler()\n",
    "X_test=X_test_scaler.fit_transform(X_test)\n",
    "y_test=y_test_scaler.fit_transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "#to scale by the train scaler\n",
    "# X_test=X_scaler.transform(X_test)\n",
    "# y_test=y_scaler.transform(np.array(y_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_NN_features():\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0001))\n",
    "    model.fit(X_train, y_train,epochs=40,batch_size=8,validation_split=0.2)\n",
    "    nnfeatures=model.predict(X_train)\n",
    "    return nnfeatures , model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "61/61 [==============================] - 1s 6ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 2/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 3/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 4/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 5/40\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 6/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 7/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 8/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 9/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 10/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 11/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 12/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 13/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 14/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 15/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 16/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 17/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 18/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 19/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 20/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 21/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 22/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 23/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 24/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 25/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 26/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 27/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 28/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 29/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 30/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 31/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 32/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 33/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 34/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 35/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 36/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 37/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 38/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 39/40\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "Epoch 40/40\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1556 - val_loss: 1.0866\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "df_train , model =_create_NN_features()\n",
    "df_test=model.predict(X_test)\n",
    "\n",
    "# df_train=X_scaler.transform(df_train)\n",
    "# df_test=X_scaler.transform(df_test)\n",
    "\n",
    "# reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "# models, predictions = reg.fit(df_train, df_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3065837041798538"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()  #using linear kernel for SVM regressor \n",
    "regressor.fit(df_train, y_train)  \n",
    "y_pred = regressor.predict(df_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n"
     ]
    }
   ],
   "source": [
    "train , test=_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to find outliers using IQR\n",
    "\n",
    "def find_outliers_IQR(df):\n",
    "\n",
    "   q1=df.quantile(0.25)\n",
    "\n",
    "   q3=df.quantile(0.75)\n",
    "\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n"
     ]
    }
   ],
   "source": [
    "train , test=_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 10) (605, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=_prepare_data(train)\n",
    "X_train,y_train , X_scaler , y_scaler=_scale_data(X_train,y_train)\n",
    "\n",
    "X_test,y_test=_prepare_data(test)\n",
    "# to set a new scaler\n",
    "# X_test_scaler=StandardScaler()\n",
    "# y_test_scaler=StandardScaler()\n",
    "# X_test=X_test_scaler.fit_transform(X_test)\n",
    "# y_test=y_test_scaler.fit_transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "#to scale by the train scaler\n",
    "X_test=X_scaler.transform(X_test)\n",
    "y_test=y_scaler.transform(np.array(y_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.457210958601155"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor()  #using linear kernel for SVM regressor \n",
    "regressor.fit(X_train, y_train)  \n",
    "y_pred = regressor.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 42/42 [00:15<00:00,  2.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANSACRegressor</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuantileRegressor</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>-1.51</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>-1.91</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>-2.05</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>-2.29</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>-4.08</td>\n",
       "      <td>-3.97</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>-5.58</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>-8265594693253987513252479762432.00</td>\n",
       "      <td>-8085907852096292475194788806656.00</td>\n",
       "      <td>2503217600513673.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Adjusted R-Squared  \\\n",
       "Model                                                               \n",
       "SVR                                                          0.35   \n",
       "NuSVR                                                        0.34   \n",
       "HuberRegressor                                               0.34   \n",
       "LinearSVR                                                    0.33   \n",
       "RANSACRegressor                                              0.24   \n",
       "OrthogonalMatchingPursuit                                    0.23   \n",
       "TweedieRegressor                                             0.22   \n",
       "ElasticNetCV                                                 0.22   \n",
       "LassoCV                                                      0.21   \n",
       "LassoLarsCV                                                  0.21   \n",
       "BayesianRidge                                                0.19   \n",
       "LassoLarsIC                                                  0.19   \n",
       "RidgeCV                                                      0.18   \n",
       "SGDRegressor                                                 0.18   \n",
       "LGBMRegressor                                                0.18   \n",
       "Ridge                                                        0.18   \n",
       "KernelRidge                                                  0.18   \n",
       "LinearRegression                                             0.18   \n",
       "TransformedTargetRegressor                                   0.18   \n",
       "OrthogonalMatchingPursuitCV                                  0.16   \n",
       "HistGradientBoostingRegressor                                0.15   \n",
       "MLPRegressor                                                 0.10   \n",
       "GaussianProcessRegressor                                     0.02   \n",
       "ElasticNet                                                  -0.02   \n",
       "LassoLars                                                   -0.02   \n",
       "Lasso                                                       -0.02   \n",
       "DummyRegressor                                              -0.02   \n",
       "QuantileRegressor                                           -0.04   \n",
       "KNeighborsRegressor                                         -0.10   \n",
       "PassiveAggressiveRegressor                                  -0.22   \n",
       "ExtraTreesRegressor                                         -0.62   \n",
       "LarsCV                                                      -0.74   \n",
       "RandomForestRegressor                                       -1.00   \n",
       "BaggingRegressor                                            -1.51   \n",
       "AdaBoostRegressor                                           -1.91   \n",
       "DecisionTreeRegressor                                       -2.05   \n",
       "GradientBoostingRegressor                                   -2.29   \n",
       "XGBRegressor                                                -4.08   \n",
       "ExtraTreeRegressor                                          -5.58   \n",
       "Lars                          -8265594693253987513252479762432.00   \n",
       "\n",
       "                                                        R-Squared  \\\n",
       "Model                                                               \n",
       "SVR                                                          0.36   \n",
       "NuSVR                                                        0.36   \n",
       "HuberRegressor                                               0.35   \n",
       "LinearSVR                                                    0.34   \n",
       "RANSACRegressor                                              0.26   \n",
       "OrthogonalMatchingPursuit                                    0.25   \n",
       "TweedieRegressor                                             0.24   \n",
       "ElasticNetCV                                                 0.23   \n",
       "LassoCV                                                      0.23   \n",
       "LassoLarsCV                                                  0.23   \n",
       "BayesianRidge                                                0.21   \n",
       "LassoLarsIC                                                  0.20   \n",
       "RidgeCV                                                      0.20   \n",
       "SGDRegressor                                                 0.20   \n",
       "LGBMRegressor                                                0.20   \n",
       "Ridge                                                        0.20   \n",
       "KernelRidge                                                  0.20   \n",
       "LinearRegression                                             0.20   \n",
       "TransformedTargetRegressor                                   0.20   \n",
       "OrthogonalMatchingPursuitCV                                  0.17   \n",
       "HistGradientBoostingRegressor                                0.17   \n",
       "MLPRegressor                                                 0.12   \n",
       "GaussianProcessRegressor                                     0.04   \n",
       "ElasticNet                                                  -0.00   \n",
       "LassoLars                                                   -0.00   \n",
       "Lasso                                                       -0.00   \n",
       "DummyRegressor                                              -0.00   \n",
       "QuantileRegressor                                           -0.02   \n",
       "KNeighborsRegressor                                         -0.07   \n",
       "PassiveAggressiveRegressor                                  -0.19   \n",
       "ExtraTreesRegressor                                         -0.58   \n",
       "LarsCV                                                      -0.70   \n",
       "RandomForestRegressor                                       -0.96   \n",
       "BaggingRegressor                                            -1.46   \n",
       "AdaBoostRegressor                                           -1.85   \n",
       "DecisionTreeRegressor                                       -1.98   \n",
       "GradientBoostingRegressor                                   -2.22   \n",
       "XGBRegressor                                                -3.97   \n",
       "ExtraTreeRegressor                                          -5.44   \n",
       "Lars                          -8085907852096292475194788806656.00   \n",
       "\n",
       "                                             RMSE  Time Taken  \n",
       "Model                                                          \n",
       "SVR                                          0.70        0.04  \n",
       "NuSVR                                        0.70        0.10  \n",
       "HuberRegressor                               0.71        0.02  \n",
       "LinearSVR                                    0.71        0.02  \n",
       "RANSACRegressor                              0.76        0.20  \n",
       "OrthogonalMatchingPursuit                    0.76        0.02  \n",
       "TweedieRegressor                             0.77        0.01  \n",
       "ElasticNetCV                                 0.77        0.10  \n",
       "LassoCV                                      0.77        0.09  \n",
       "LassoLarsCV                                  0.77        0.03  \n",
       "BayesianRidge                                0.78        0.01  \n",
       "LassoLarsIC                                  0.79        0.02  \n",
       "RidgeCV                                      0.79        0.01  \n",
       "SGDRegressor                                 0.79        0.01  \n",
       "LGBMRegressor                                0.79        0.12  \n",
       "Ridge                                        0.79        0.01  \n",
       "KernelRidge                                  0.79        0.07  \n",
       "LinearRegression                             0.79        0.01  \n",
       "TransformedTargetRegressor                   0.79        0.01  \n",
       "OrthogonalMatchingPursuitCV                  0.80        0.02  \n",
       "HistGradientBoostingRegressor                0.80        0.72  \n",
       "MLPRegressor                                 0.82        1.09  \n",
       "GaussianProcessRegressor                     0.86        0.17  \n",
       "ElasticNet                                   0.88        0.01  \n",
       "LassoLars                                    0.88        0.02  \n",
       "Lasso                                        0.88        0.01  \n",
       "DummyRegressor                               0.88        0.01  \n",
       "QuantileRegressor                            0.89       10.75  \n",
       "KNeighborsRegressor                          0.91        0.02  \n",
       "PassiveAggressiveRegressor                   0.96        0.01  \n",
       "ExtraTreesRegressor                          1.11        0.32  \n",
       "LarsCV                                       1.15        0.06  \n",
       "RandomForestRegressor                        1.23        0.44  \n",
       "BaggingRegressor                             1.38        0.07  \n",
       "AdaBoostRegressor                            1.49        0.23  \n",
       "DecisionTreeRegressor                        1.52        0.02  \n",
       "GradientBoostingRegressor                    1.58        0.29  \n",
       "XGBRegressor                                 1.96        0.15  \n",
       "ExtraTreeRegressor                           2.23        0.02  \n",
       "Lars                          2503217600513673.00        0.05  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=LazyRegressor()\n",
    "models=reg.fit(X_train, X_test, y_train, y_test)\n",
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
