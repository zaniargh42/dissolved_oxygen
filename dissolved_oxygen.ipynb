{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense  ,Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor ,ExtraTreesRegressor ,GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data():\n",
    "    train=pd.read_excel('Data set - Tisa.xlsx',sheet_name='Training set 2011-2015')\n",
    "    train.columns=['temperature', 'solids', 'dissolved_oxygen', 'pH','electrical', 'NH4', 'NO2', 'NO3', 'TN', 'PO4P', 'BOD5']\n",
    "    train=train.drop(list(train[train.isna().any(axis=1)].index),axis=0)\n",
    "\n",
    "    test=pd.read_excel('Data set - Tisa.xlsx',sheet_name='Testing set 2016-2019 ')\n",
    "    test.columns=['temperature', 'solids', 'dissolved_oxygen', 'pH','electrical', 'NH4', 'NO2', 'NO3', 'TN', 'PO4P', 'BOD5']\n",
    "    test=test.drop(list(test[test.isna().any(axis=1)].index),axis=0)\n",
    "\n",
    "    print(train.shape,test.shape)\n",
    "    return train , test\n",
    "\n",
    "def _prepare_data(data):\n",
    "    X_train=data.drop(['dissolved_oxygen'],axis=1)\n",
    "    y_train=data.dissolved_oxygen\n",
    "    return X_train , y_train\n",
    "\n",
    "\n",
    "def _calc_corr(y_test,y_pred,sqrt=False):\n",
    "    res=pd.DataFrame(y_pred,columns=['pred'])\n",
    "    res['real']=y_test\n",
    "    if sqrt:\n",
    "        return np.sqrt(res.corr())\n",
    "    else: return res.corr()\n",
    "\n",
    "def _zscore(df):\n",
    "    df_scaled=zscore(df,axis=1)\n",
    "    return df_scaled\n",
    "\n",
    "def _scale_data(X_train,y_train,X_test,y_test,same=False):\n",
    "    X_scaler=StandardScaler()\n",
    "    X_train_scaled=X_scaler.fit_transform(X_train)\n",
    "    y_scaler=StandardScaler()\n",
    "    y_train_scaled=y_scaler.fit_transform(np.array(y_train).reshape(-1,1))    \n",
    "    if same:\n",
    "        X_test_scaled=X_scaler.transform(X_test)\n",
    "        y_test_scaled=y_scaler.transform(np.array(y_test).reshape(-1,1))\n",
    "\n",
    "        return X_train_scaled,y_train_scaled,X_test_scaled,y_test_scaled\n",
    "    else:\n",
    "        X_test_scaler=StandardScaler()\n",
    "        y_test_scaler=StandardScaler()\n",
    "        X_test_scaled=X_test_scaler.fit_transform(X_test)\n",
    "        y_test_scaled=y_test_scaler.fit_transform(np.array(y_test).reshape(-1,1))  \n",
    "        return X_train_scaled,y_train_scaled,X_test_scaled,y_test_scaled    \n",
    "\n",
    "#create a function to find outliers using IQR\n",
    "def find_outliers_IQR(df):\n",
    "   q1=df.quantile(0.25)\n",
    "   q3=df.quantile(0.75)\n",
    "   IQR=q3-q1\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "   return outliers\n",
    "\n",
    "def _relative_error(y_test,y_pred):\n",
    "    re=[]\n",
    "    for item in range(len(y_test)):\n",
    "        re.append((abs(y_test[item]-y_pred[item])/y_test[item])*100)\n",
    "    return np.mean(re)\n",
    "\n",
    "def _errors(y_test, y_pred):\n",
    "    mse=mean_squared_error(y_test, y_pred)\n",
    "    rmse=np.sqrt(mse)\n",
    "    relative=_relative_error(y_test,y_pred)\n",
    "    mae=mean_absolute_error(y_test, y_pred)\n",
    "    corr=_calc_corr(y_test,y_pred,sqrt=False)\n",
    "    sq_corr=r2_score(y_test,y_pred)\n",
    "    print(f\"mse: {mse}\\nrmse: {rmse}\\nrelative: {relative} %\\nmae: {mae}\\ncorr:{corr['real']['pred']}\\nsq_corr:{sq_corr}\\n\")\n",
    "    return [mse,rmse,relative,mae,corr['real']['pred'],sq_corr]\n",
    "\n",
    "def _drop_outliers(train,test,drop_test=False):\n",
    "    train=train[find_outliers_IQR(train).isna()].dropna()\n",
    "    if drop_test:\n",
    "        test=test[find_outliers_IQR(test).isna()].dropna()\n",
    "        print(train.shape,test.shape)\n",
    "        return train,test\n",
    "    else:\n",
    "        print(train.shape,test.shape)\n",
    "        return train,test\n",
    "\n",
    "def _augment_data(df,n=1000):\n",
    "    #creating fake data\n",
    "    fake=pd.DataFrame([list(range(1,len(df.columns)+1))],columns=df.columns,index=range(n))\n",
    "    fake.columns\n",
    "    for item in fake.columns:\n",
    "        fake[item]=np.random.random(n)\n",
    "    fake['fake']=np.ones(len(fake))\n",
    "    # concatenate fake and real data\n",
    "    df['fake']=np.zeros(len(df))\n",
    "    temp=pd.concat([fake,df],axis=0)\n",
    "    temp.reset_index(drop=True,inplace=True)\n",
    "    # Augment data\n",
    "    X=temp.drop(['fake'],axis=1)\n",
    "    y=temp.fake\n",
    "    # transform the dataset\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    X['fake']=y\n",
    "    temp=X[X.fake==0]\n",
    "    temp.drop(['fake'],axis=1,inplace=True)\n",
    "    temp.reset_index(drop=True,inplace=True)\n",
    "    return temp\n",
    "\n",
    "def _dataset_type(train, test, type='ds1'):\n",
    "    if type=='ds1':\n",
    "        train.drop(['solids','NO2', 'NO3', 'TN','BOD5'],axis=1,inplace=True)\n",
    "        test.drop(['solids','NO2', 'NO3', 'TN','BOD5'],axis=1,inplace=True)\n",
    "        return train , test\n",
    "    elif type=='ds2':\n",
    "        train.drop(['solids','NO2', 'NO3', 'TN','BOD5','NH4'],axis=1,inplace=True)\n",
    "        test.drop(['solids','NO2', 'NO3', 'TN','BOD5','NH4'],axis=1,inplace=True)\n",
    "        return train , test\n",
    "    elif type=='ds3':\n",
    "        train.drop(['solids','NO2', 'NO3', 'TN','BOD5','NH4','electrical'],axis=1,inplace=True)\n",
    "        test.drop(['solids','NO2', 'NO3', 'TN','BOD5','NH4','electrical'],axis=1,inplace=True) \n",
    "        return train , test  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n",
      "(466, 6) (339, 6)\n",
      "(466, 6)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "train,test=_load_data()\n",
    "\n",
    "# dataset type\n",
    "train , test = _dataset_type(train, test,type='ds1')\n",
    "\n",
    "# drop outliers\n",
    "train,test=_drop_outliers(train , test,drop_test=True)\n",
    "\n",
    "# z score standard \n",
    "train=_zscore(train)\n",
    "test =_zscore(test)\n",
    "\n",
    "# Augment data\n",
    "\n",
    "# train=_augment_data(train,n=1000)\n",
    "print(train.shape)\n",
    "\n",
    "# train test and x y split\n",
    "X_train , y_train=_prepare_data(train)\n",
    "X_test , y_test =_prepare_data(test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint\n",
    "# - ExtraTreeRegressor\n",
    "# - this model has r2=0.741\n",
    "# joblib.dump(model, 'extratreereg.joblib.pkl', compress=9)\n",
    "model= joblib.load('extratreereg.joblib.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.3589497508925861\n",
      "rmse: 0.5991241531540737\n",
      "relative: 6846.61064982145 %\n",
      "mae: 0.42801483805367946\n",
      "corr:0.802122713418148\n",
      "sq_corr:0.6410502491074139\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3589497508925861,\n",
       " 0.5991241531540737,\n",
       " 6846.61064982145,\n",
       " 0.42801483805367946,\n",
       " 0.802122713418148,\n",
       " 0.6410502491074139]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=KNeighborsRegressor()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test.values)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.557946984543674\n",
      "rmse: 0.7469584891703648\n",
      "relative: 3774.6978308569 %\n",
      "mae: 0.5345342197800231\n",
      "corr:0.7196925936858217\n",
      "sq_corr:0.442053015456326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.557946984543674,\n",
       " 0.7469584891703648,\n",
       " 3774.6978308569,\n",
       " 0.5345342197800231,\n",
       " 0.7196925936858217,\n",
       " 0.442053015456326]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DecisionTreeRegressor()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.32244888126934396\n",
      "rmse: 0.5678458252636396\n",
      "relative: 5827.3243782786085 %\n",
      "mae: 0.38867983259842404\n",
      "corr:0.8242536102962836\n",
      "sq_corr:0.677551118730656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32244888126934396,\n",
       " 0.5678458252636396,\n",
       " 5827.3243782786085,\n",
       " 0.38867983259842404,\n",
       " 0.8242536102962836,\n",
       " 0.677551118730656]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RandomForestRegressor()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.3079007120670025\n",
      "rmse: 0.5548880175918403\n",
      "relative: 5360.5035112858695 %\n",
      "mae: 0.3775573511961226\n",
      "corr:0.8355811106024965\n",
      "sq_corr:0.6920992879329975\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3079007120670025,\n",
       " 0.5548880175918403,\n",
       " 5360.5035112858695,\n",
       " 0.3775573511961226,\n",
       " 0.8355811106024965,\n",
       " 0.6920992879329975]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GradientBoostingRegressor()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.3069057019868111\n",
      "rmse: 0.5539907056863058\n",
      "relative: 4891.9485710719 %\n",
      "mae: 0.3713532004630767\n",
      "corr:0.8337166424738669\n",
      "sq_corr:0.6930942980131889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but SVR was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3069057019868111,\n",
       " 0.5539907056863058,\n",
       " 4891.9485710719,\n",
       " 0.3713532004630767,\n",
       " 0.8337166424738669,\n",
       " 0.6930942980131889]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=SVR()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.2866537827814137\n",
      "rmse: 0.5354005816035444\n",
      "relative: 5649.215208292452 %\n",
      "mae: 0.36125869926666976\n",
      "corr:0.8458438823268267\n",
      "sq_corr:0.7133462172185863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zaniargh42/.local/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but MLPRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2866537827814137,\n",
       " 0.5354005816035444,\n",
       " 5649.215208292452,\n",
       " 0.36125869926666976,\n",
       " 0.8458438823268267,\n",
       " 0.7133462172185863]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=MLPRegressor()\n",
    "model.fit(X_train.values,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "_errors(y_test.values, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16/16 [==============================] - 1s 19ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0000 - val_loss: 0.9999\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9999 - val_loss: 0.9999\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9999 - val_loss: 0.9999\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9998 - val_loss: 0.9998\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.9998 - val_loss: 0.9997\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9996 - val_loss: 0.9995\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9994 - val_loss: 0.9993\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.9992 - val_loss: 0.9990\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9988 - val_loss: 0.9986\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9984 - val_loss: 0.9981\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9978 - val_loss: 0.9974\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9969 - val_loss: 0.9965\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9959 - val_loss: 0.9954\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9947 - val_loss: 0.9939\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9930 - val_loss: 0.9921\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9910 - val_loss: 0.9900\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9886 - val_loss: 0.9872\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9854 - val_loss: 0.9841\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9820 - val_loss: 0.9803\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9778 - val_loss: 0.9759\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9731 - val_loss: 0.9707\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9674 - val_loss: 0.9650\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9611 - val_loss: 0.9583\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9538 - val_loss: 0.9508\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9458 - val_loss: 0.9419\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9362 - val_loss: 0.9327\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9259 - val_loss: 0.9225\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9148 - val_loss: 0.9107\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.9023 - val_loss: 0.8980\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8886 - val_loss: 0.8841\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8744 - val_loss: 0.8695\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8584 - val_loss: 0.8549\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8432 - val_loss: 0.8389\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8266 - val_loss: 0.8229\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8098 - val_loss: 0.8061\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7928 - val_loss: 0.7896\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7751 - val_loss: 0.7732\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7583 - val_loss: 0.7565\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7411 - val_loss: 0.7409\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7253 - val_loss: 0.7255\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7095 - val_loss: 0.7111\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6949 - val_loss: 0.6977\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6814 - val_loss: 0.6844\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6686 - val_loss: 0.6724\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6567 - val_loss: 0.6610\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6456 - val_loss: 0.6509\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6353 - val_loss: 0.6416\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6261 - val_loss: 0.6329\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6168 - val_loss: 0.6244\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6080 - val_loss: 0.6166\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6001 - val_loss: 0.6088\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5920 - val_loss: 0.6014\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5844 - val_loss: 0.5940\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5770 - val_loss: 0.5868\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5695 - val_loss: 0.5797\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5622 - val_loss: 0.5726\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5547 - val_loss: 0.5657\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5475 - val_loss: 0.5590\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5399 - val_loss: 0.5517\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5327 - val_loss: 0.5438\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5251 - val_loss: 0.5372\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5177 - val_loss: 0.5298\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5098 - val_loss: 0.5230\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5027 - val_loss: 0.5157\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4947 - val_loss: 0.5088\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4874 - val_loss: 0.5017\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4798 - val_loss: 0.4953\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4723 - val_loss: 0.4888\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4647 - val_loss: 0.4817\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4571 - val_loss: 0.4747\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4494 - val_loss: 0.4684\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4422 - val_loss: 0.4617\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4345 - val_loss: 0.4552\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4276 - val_loss: 0.4494\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4198 - val_loss: 0.4430\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4129 - val_loss: 0.4361\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4054 - val_loss: 0.4302\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3985 - val_loss: 0.4240\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3918 - val_loss: 0.4187\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3847 - val_loss: 0.4129\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3780 - val_loss: 0.4074\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3716 - val_loss: 0.4025\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3656 - val_loss: 0.3976\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3597 - val_loss: 0.3922\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3538 - val_loss: 0.3880\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3481 - val_loss: 0.3839\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3424 - val_loss: 0.3800\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3372 - val_loss: 0.3756\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3326 - val_loss: 0.3724\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3271 - val_loss: 0.3683\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3226 - val_loss: 0.3646\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3180 - val_loss: 0.3620\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3141 - val_loss: 0.3592\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3098 - val_loss: 0.3560\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3063 - val_loss: 0.3543\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3027 - val_loss: 0.3519\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2991 - val_loss: 0.3494\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2958 - val_loss: 0.3472\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2929 - val_loss: 0.3464\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2900 - val_loss: 0.3449\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2872 - val_loss: 0.3424\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2848 - val_loss: 0.3408\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2821 - val_loss: 0.3401\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2799 - val_loss: 0.3391\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2779 - val_loss: 0.3391\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2758 - val_loss: 0.3383\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2740 - val_loss: 0.3366\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2721 - val_loss: 0.3363\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2704 - val_loss: 0.3356\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2689 - val_loss: 0.3345\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2673 - val_loss: 0.3334\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2660 - val_loss: 0.3331\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2645 - val_loss: 0.3323\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2634 - val_loss: 0.3313\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2622 - val_loss: 0.3313\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2610 - val_loss: 0.3306\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2603 - val_loss: 0.3314\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2590 - val_loss: 0.3301\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2581 - val_loss: 0.3296\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2572 - val_loss: 0.3289\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2564 - val_loss: 0.3292\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2556 - val_loss: 0.3290\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2547 - val_loss: 0.3282\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2540 - val_loss: 0.3276\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2534 - val_loss: 0.3271\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2527 - val_loss: 0.3269\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2522 - val_loss: 0.3263\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2516 - val_loss: 0.3262\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2510 - val_loss: 0.3271\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2506 - val_loss: 0.3264\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2497 - val_loss: 0.3263\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2491 - val_loss: 0.3260\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2485 - val_loss: 0.3251\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2482 - val_loss: 0.3242\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2479 - val_loss: 0.3237\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2474 - val_loss: 0.3253\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2468 - val_loss: 0.3240\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2461 - val_loss: 0.3243\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2457 - val_loss: 0.3235\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2452 - val_loss: 0.3239\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2449 - val_loss: 0.3233\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2444 - val_loss: 0.3232\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2441 - val_loss: 0.3226\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2436 - val_loss: 0.3226\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2431 - val_loss: 0.3220\n",
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.32202964762018377\n",
      "rmse: 0.5674765612958687\n",
      "relative: 5749.03857421875 %\n",
      "mae: 0.3831345308906208\n",
      "corr:0.824351058346981\n",
      "sq_corr:0.6779703523798162\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32202964762018377,\n",
       " 0.5674765612958687,\n",
       " 5749.0386,\n",
       " 0.3831345308906208,\n",
       " 0.824351058346981,\n",
       " 0.6779703523798162]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _compile_model(X_train,y_train,X_test,y_test,epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_shape=(X_train[0].shape[0],),kernel_initializer='RandomNormal', activation='relu'))\n",
    "    model.add(Dense(17,kernel_initializer='RandomNormal', activation='relu'))\n",
    "    model.add(Dense(10,kernel_initializer='RandomNormal', activation='relu'))\n",
    "    model.add(Dense(7,kernel_initializer='RandomNormal', activation='relu'))\n",
    "    model.add(Dense(1,kernel_initializer='RandomNormal',activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(0.0001))\n",
    "    model.fit(X_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\n",
    "    \n",
    "    history=model.history\n",
    "    y_pred=model.predict(X_test)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    return history , r2 , y_pred,model\n",
    "history , r2 , y_pred,model=_compile_model(X_train.values,y_train.values,X_test.values,y_test.values,150,30)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.show()\n",
    "_errors(y_test.values,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.9994 - val_loss: 0.9971\n",
      "Epoch 2/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.9653 - val_loss: 0.9093\n",
      "Epoch 3/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.7286 - val_loss: 0.5804\n",
      "Epoch 4/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 0.4047\n",
      "Epoch 5/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.2792 - val_loss: 0.3753\n",
      "Epoch 6/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.2482 - val_loss: 0.3644\n",
      "Epoch 7/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.3589\n",
      "Epoch 8/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 0.3550\n",
      "Epoch 9/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.2049 - val_loss: 0.3540\n",
      "Epoch 10/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 0.3518\n",
      "Epoch 11/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1891 - val_loss: 0.3524\n",
      "Epoch 12/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1828 - val_loss: 0.3495\n",
      "Epoch 13/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1777 - val_loss: 0.3487\n",
      "Epoch 14/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 0.3525\n",
      "Epoch 15/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1690 - val_loss: 0.3508\n",
      "Epoch 16/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1655 - val_loss: 0.3487\n",
      "Epoch 17/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 0.3494\n",
      "Epoch 18/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1600 - val_loss: 0.3463\n",
      "Epoch 19/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 0.3461\n",
      "Epoch 20/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 0.3445\n",
      "Epoch 21/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 0.3422\n",
      "Epoch 22/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1520 - val_loss: 0.3395\n",
      "Epoch 23/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 0.3376\n",
      "Epoch 24/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1487 - val_loss: 0.3345\n",
      "Epoch 25/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 0.3323\n",
      "Epoch 26/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1462 - val_loss: 0.3277\n",
      "Epoch 27/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1445 - val_loss: 0.3270\n",
      "Epoch 28/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 0.3255\n",
      "Epoch 29/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1419 - val_loss: 0.3231\n",
      "Epoch 30/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1406 - val_loss: 0.3197\n",
      "Epoch 31/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 0.3172\n",
      "Epoch 32/100\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1380 - val_loss: 0.3159\n",
      "Epoch 33/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 0.3110\n",
      "Epoch 34/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1347 - val_loss: 0.3123\n",
      "Epoch 35/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 0.3089\n",
      "Epoch 36/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1322 - val_loss: 0.3056\n",
      "Epoch 37/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 0.3051\n",
      "Epoch 38/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1295 - val_loss: 0.3037\n",
      "Epoch 39/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1285 - val_loss: 0.3038\n",
      "Epoch 40/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.3004\n",
      "Epoch 41/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 0.3011\n",
      "Epoch 42/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.2996\n",
      "Epoch 43/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.3009\n",
      "Epoch 44/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.3003\n",
      "Epoch 45/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.2990\n",
      "Epoch 46/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.2965\n",
      "Epoch 47/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 0.2975\n",
      "Epoch 48/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 0.2980\n",
      "Epoch 49/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1210 - val_loss: 0.2973\n",
      "Epoch 50/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 0.2946\n",
      "Epoch 51/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1199 - val_loss: 0.2957\n",
      "Epoch 52/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 0.2948\n",
      "Epoch 53/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 0.2951\n",
      "Epoch 54/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 0.2943\n",
      "Epoch 55/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1183 - val_loss: 0.2929\n",
      "Epoch 56/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1175 - val_loss: 0.2938\n",
      "Epoch 57/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1175 - val_loss: 0.2919\n",
      "Epoch 58/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1171 - val_loss: 0.2907\n",
      "Epoch 59/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1161 - val_loss: 0.2912\n",
      "Epoch 60/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1163 - val_loss: 0.2920\n",
      "Epoch 61/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1156 - val_loss: 0.2882\n",
      "Epoch 62/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 0.2894\n",
      "Epoch 63/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1152 - val_loss: 0.2893\n",
      "Epoch 64/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1148 - val_loss: 0.2882\n",
      "Epoch 65/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 0.2910\n",
      "Epoch 66/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 0.2883\n",
      "Epoch 67/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 0.2884\n",
      "Epoch 68/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1135 - val_loss: 0.2895\n",
      "Epoch 69/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.2909\n",
      "Epoch 70/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 0.2881\n",
      "Epoch 71/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1127 - val_loss: 0.2881\n",
      "Epoch 72/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 0.2846\n",
      "Epoch 73/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 0.2865\n",
      "Epoch 74/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 0.2869\n",
      "Epoch 75/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.2855\n",
      "Epoch 76/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 0.1112 - val_loss: 0.2905\n",
      "Epoch 77/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1108 - val_loss: 0.2909\n",
      "Epoch 78/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.2850\n",
      "Epoch 79/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 0.1108 - val_loss: 0.2868\n",
      "Epoch 80/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1102 - val_loss: 0.2838\n",
      "Epoch 81/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1102 - val_loss: 0.2883\n",
      "Epoch 82/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1097 - val_loss: 0.2884\n",
      "Epoch 83/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 0.2908\n",
      "Epoch 84/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.2863\n",
      "Epoch 85/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 0.2846\n",
      "Epoch 86/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 0.1087 - val_loss: 0.2864\n",
      "Epoch 87/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 0.2836\n",
      "Epoch 88/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.2865\n",
      "Epoch 89/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1078 - val_loss: 0.2862\n",
      "Epoch 90/100\n",
      "167/167 [==============================] - 1s 3ms/step - loss: 0.1080 - val_loss: 0.2894\n",
      "Epoch 91/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 0.2842\n",
      "Epoch 92/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 0.2854\n",
      "Epoch 93/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 0.2893\n",
      "Epoch 94/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1070 - val_loss: 0.2905\n",
      "Epoch 95/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.2871\n",
      "Epoch 96/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.2864\n",
      "Epoch 97/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1063 - val_loss: 0.2868\n",
      "Epoch 98/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.2924\n",
      "Epoch 99/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 0.2897\n",
      "Epoch 100/100\n",
      "167/167 [==============================] - 0s 3ms/step - loss: 0.1059 - val_loss: 0.2919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6087733d0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(15, input_shape=(X_train[0].shape[0],),kernel_initializer='RandomNormal', activation='relu'))\n",
    "model.add(Dense(17,kernel_initializer='RandomNormal', activation='relu'))\n",
    "model.add(Dense(20,kernel_initializer='RandomNormal', activation='relu'))\n",
    "model.add(Dense(30,kernel_initializer='RandomNormal', activation='relu'))\n",
    "model.add(Dense(40,kernel_initializer='RandomNormal',activation='linear'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(0.0001))\n",
    "model.fit(X_train, y_train,epochs=100,batch_size=30,validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 40)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnf_train=model.predict(X_train)\n",
    "nnf_test=model.predict(X_test)\n",
    "nnf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 11) (461, 11)\n",
      "(510, 4) (383, 4)\n"
     ]
    }
   ],
   "source": [
    "train,test=_load_data()\n",
    "# dataset type\n",
    "train , test = _dataset_type(train, test,type='ds3')\n",
    "# drop outliers\n",
    "train,test=_drop_outliers(train , test,drop_test=True)\n",
    "\n",
    "mean=[]\n",
    "std=[]\n",
    "for item in test.index:\n",
    "    mean.append(np.mean(list(test.loc[item,:])))\n",
    "    std.append(np.std(list(test.loc[item,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['mean']=mean\n",
    "test['std']=std\n",
    "test.to_excel('ds1_mean.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
